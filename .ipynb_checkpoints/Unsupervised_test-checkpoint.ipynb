{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70ae998b-1fa9-40ac-88f5-0294d36c8526",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "# Libraries for VGG \n",
    "from tensorflow.keras.layers import Dense, Conv2D\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import MaxPooling2D, Input\n",
    "from tensorflow.keras.layers import Flatten, AveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.io as io\n",
    "from skimage.transform import resize, rotate\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9d00d9a-127d-4ca5-a70d-4672ad33dc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A to E are standard VGG backbones\n",
    "# F was customized for IIC\n",
    "# G is experimental\n",
    "cfg = {\n",
    "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M',512, 512, 512, 512, 'M'],\n",
    "    'F': [64, 'M', 128, 'M', 256, 'M', 512],\n",
    "    'G': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'A'],\n",
    "}\n",
    "\n",
    "class VGG:\n",
    "    def __init__(self, cfg, input_shape=(28, 28, 1)):\n",
    "        \"\"\"VGG network model creator to be used as backbone\n",
    "            feature extractor\n",
    "\n",
    "        Arguments:\n",
    "            cfg (dict): Summarizes the network configuration\n",
    "            input_shape (list): Input image dims\n",
    "        \"\"\"\n",
    "        self.cfg = cfg\n",
    "        self.input_shape = input_shape\n",
    "        self._model = None\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Model builder uses a helper function\n",
    "            make_layers to read the config dict and\n",
    "            create a VGG network model\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=self.input_shape, name='x')\n",
    "        x = VGG.make_layers(self.cfg, inputs)\n",
    "        self._model = Model(inputs, x, name='VGG')\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        return self._model\n",
    "\n",
    "    @staticmethod\n",
    "    def make_layers(cfg,\n",
    "                    inputs, \n",
    "                    batch_norm=True, \n",
    "                    in_channels=1):\n",
    "        \"\"\"Helper function to ease the creation of VGG\n",
    "            network model\n",
    "\n",
    "        Arguments:\n",
    "            cfg (dict): Summarizes the network layer \n",
    "                configuration\n",
    "            inputs (tensor): Input from previous layer\n",
    "            batch_norm (Bool): Whether to use batch norm\n",
    "                between Conv2D and ReLU\n",
    "            in_channel (int): Number of input channels\n",
    "        \"\"\"\n",
    "        x = inputs\n",
    "        for layer in cfg:\n",
    "            if layer == 'M':\n",
    "                x = MaxPooling2D()(x)\n",
    "            elif layer == 'A':\n",
    "                x = AveragePooling2D(pool_size=3)(x)\n",
    "            else:\n",
    "                x = Conv2D(layer,\n",
    "                           kernel_size=3,\n",
    "                           padding='same',\n",
    "                           kernel_initializer='he_normal'\n",
    "                           )(x)\n",
    "                if batch_norm:\n",
    "                    x = BatchNormalization()(x)\n",
    "                x = Activation('relu')(x)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e976d87a-d593-496f-a62e-cc6dbb53cf2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"VGG\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "x (InputLayer)               [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 28, 28, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 14, 14, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 7, 7, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 7, 7, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 3, 3, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 3, 3, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 3, 3, 512)         0         \n",
      "=================================================================\n",
      "Total params: 1,553,664\n",
      "Trainable params: 1,551,744\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build backbone\n",
    "backbone = VGG(cfg['F'])\n",
    "backbone.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9a88766-0452-4572-8c94-290f05e7d7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = mnist #Dataset to use\n",
    "epochs = 10 #Number of epochs to train\n",
    "batch_size = 512 #Train batch size\n",
    "heads = 1 #Number of heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b26bdb0a-4efa-4133-9726-6054b6baea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-load test data for evaluation\n",
    "(_, _), (x_test, y_test) = dataset.load_data()\n",
    "x_test = np.reshape(x_test,[-1, x_test.shape[1], x_test.shape[2], 1])\n",
    "x_test = x_test.astype('float32') / 255\n",
    "x_eval = np.zeros([x_test.shape[0], *train_gen.input_shape])\n",
    "for i in range(x_eval.shape[0]):\n",
    "    x_eval[i] = x_test[i]\n",
    "#x_test = x_eval[0:2000]\n",
    "#y_test = y_test[0:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03872ab1-3f4d-4e37-931b-fccc532e8b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence): # Multi-threaded data generator. Each thread reads a batch of images and performs image transformation such that the image class is unaffected\n",
    "    def __init__(self, shuffle=True):\n",
    "        self.shuffle = shuffle # shuffle (Bool): Whether to shuffle the dataset before sampling or not\n",
    "        (self.data, _), (_, _) = dataset.load_data()\n",
    "        #self.data = self.data[0:2000]\n",
    "        self.n_channels = 1\n",
    "        self.input_shape = [self.data.shape[1], self.data.shape[2], self.n_channels]\n",
    "        self.n_labels = 10\n",
    "        self.indexes = [i for i in range(self.data.shape[0])]\n",
    "        # reshape and normalize input images\n",
    "        new_shape = [-1, self.data.shape[1], self.data.shape[2], self.n_channels]\n",
    "        self.data = np.reshape(self.data, new_shape)\n",
    "        self.data = self.data.astype('float32') / 255                \n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self): # Number of batches per epoch\n",
    "        return int(np.floor(len(self.indexes) / batch_size))\n",
    "\n",
    "    def __getitem__(self, index): # Image sample Indexes for the current batch\n",
    "        start_index = index * batch_size\n",
    "        end_index = (index+1) * batch_size\n",
    "        return self.__data_generation(start_index, end_index)\n",
    "\n",
    "    def on_epoch_end(self): # If opted, shuffle dataset after each epoch\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def random_crop(self, image, target_shape, crop_sizes): # Perform random crop, resize back to its target shape\n",
    "        # image (tensor): Image to crop and resize\n",
    "        # target_shape (tensor): Output shape\n",
    "        # crop_sizes (list): A list of sizes the image can be cropped\n",
    "        height, width = image.shape[0], image.shape[1]\n",
    "        crop_size_idx = np.random.randint(0, len(crop_sizes))\n",
    "        d = crop_sizes[crop_size_idx]\n",
    "        x = height - d\n",
    "        y = width - d\n",
    "        center = np.random.randint(0, 2)\n",
    "        if center:\n",
    "            dx = dy = d // 2\n",
    "        else:\n",
    "            dx = np.random.randint(0, d + 1)\n",
    "            dy = np.random.randint(0, d + 1)\n",
    "        image = image[dx:(x + dx),dy:(y + dy), :]\n",
    "        image = resize(image, target_shape)\n",
    "        return image\n",
    "    \n",
    "    def __data_generation(self, start_index, end_index): # Data generation algorithm. The method generates a batch of pair of images (original image X and transformed imaged Xbar). \n",
    "        x = self.data[self.indexes[start_index : end_index]] # Given an array of images.  the start index to retrieve a batch, the end index to retrieve a batch\n",
    "        target_shape = (x.shape[0], *self.input_shape)\n",
    "        x1 = np.zeros(target_shape)\n",
    "        x2 = np.zeros(target_shape)\n",
    "        for i in range(x.shape[0]):\n",
    "            image = x[i]\n",
    "            crop_sizes = [8 + i for i in range(0,5,2)]\n",
    "            image_bar = self.random_crop(image,target_shape[1:],crop_sizes)\n",
    "            x1[i] = image\n",
    "            x2[i] = image_bar\n",
    "        x_train = np.concatenate([x1, x2], axis=0) # for IIC, we are mostly interested in paired images X and Xbar = G(X)\n",
    "\n",
    "        y = np.zeros(len(x_train))\n",
    "        return x_train,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9e411c8-dbf0-4493-9eb0-67eb0953122d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "x (InputLayer)               [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "VGG (Functional)             (None, 3, 3, 512)         1553664   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "z_head0 (Dense)              (None, 10)                46090     \n",
      "=================================================================\n",
      "Total params: 1,599,754\n",
      "Trainable params: 1,597,834\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def mi_loss(y_true, y_pred): # Mutual information loss computed from the joint distribution matrix and the marginals\n",
    "    # y_true (tensor): Not used since this is unsupervised learning\n",
    "    # y_pred (tensor): stack of softmax predictions the latent vectors (Z and Zbar)\n",
    "    size = batch_size\n",
    "    n_labels = y_pred.shape[-1]\n",
    "    # lower half is Z\n",
    "    Z = y_pred[0: size, :]\n",
    "    Z = K.expand_dims(Z, axis=2)\n",
    "    # upper half is Zbar\n",
    "    Zbar = y_pred[size: y_pred.shape[0], :]\n",
    "    Zbar = K.expand_dims(Zbar, axis=1)\n",
    "    # compute joint distribution (Eq 10.3.2 & .3)\n",
    "    P = K.batch_dot(Z, Zbar)\n",
    "    P = K.sum(P, axis=0)\n",
    "    # enforce symmetric joint distribution (Eq 10.3.4)\n",
    "    P = (P + K.transpose(P)) / 2.0\n",
    "    # normalization of total probability to 1.0\n",
    "    P = P / K.sum(P)\n",
    "    # marginal distributions (Eq 10.3.5 & .6)\n",
    "    Pi = K.expand_dims(K.sum(P, axis=1), axis=1)\n",
    "    Pj = K.expand_dims(K.sum(P, axis=0), axis=0)\n",
    "    Pi = K.repeat_elements(Pi, rep=n_labels, axis=1)\n",
    "    Pj = K.repeat_elements(Pj, rep=n_labels, axis=0)\n",
    "    P = K.clip(P, K.epsilon(), np.finfo(float).max)\n",
    "    Pi = K.clip(Pi, K.epsilon(), np.finfo(float).max)\n",
    "    Pj = K.clip(Pj, K.epsilon(), np.finfo(float).max)\n",
    "    # negative MI loss (Eq 10.3.7)\n",
    "    neg_mi = K.sum((P * (K.log(Pi) + K.log(Pj) - K.log(P))))\n",
    "    # each head contribute 1/n_heads to the total loss\n",
    "    return neg_mi/heads\n",
    "\n",
    "train_gen = DataGenerator(shuffle=True)\n",
    "print(train_gen.data.shape)\n",
    "n_labels = train_gen.n_labels\n",
    "inputs = Input(shape=train_gen.input_shape, name='x') # Build the n_heads of the IIC model\n",
    "my_backbone = backbone.model\n",
    "x = my_backbone(inputs)\n",
    "x = Flatten()(x)\n",
    "outputs = [] # number of output heads\n",
    "for i in range(heads):\n",
    "    name = \"z_head%d\" % i\n",
    "    outputs.append(Dense(n_labels,\n",
    "                         activation='softmax',\n",
    "                         name=name)(x))\n",
    "my_model = Model(inputs, outputs, name='encoder')\n",
    "my_model.compile(optimizer=Adam(learning_rate=1e-3), loss=mi_loss)\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d5f46f6-aa50-4546-8404-1689b0abc76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.001.\n",
      "117/117 [==============================] - 483s 4s/step - loss: -0.6740\n",
      "Head 0 accuracy: 11.87%\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.001.\n",
      "117/117 [==============================] - 483s 4s/step - loss: -0.6928\n",
      "Head 0 accuracy: 17.40%, Old best accuracy: 11.87%\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.001.\n",
      "117/117 [==============================] - 468s 4s/step - loss: -0.6929\n",
      "Head 0 accuracy: 11.35%, Old best accuracy: 17.40%\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.001.\n",
      " 95/117 [=======================>......] - ETA: 1:30 - loss: -0.6932"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-599e3135c13c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Train function uses the data generator, accuracy computation, and learning rate scheduler callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m my_model.fit_generator(generator=train_gen,\n\u001b[0m\u001b[1;32m     44\u001b[0m                             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1941\u001b[0m                   \u001b[0;34m'will be removed in a future version. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m                   'Please use `Model.fit`, which supports generators.')\n\u001b[0;32m-> 1943\u001b[0;31m     return self.fit(\n\u001b[0m\u001b[1;32m   1944\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1945\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3023\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "\n",
    "def lr_schedule(epoch): # Simple learning rate scheduler // Argument: epoch (int): Which epoch \n",
    "    lr = 1e-3\n",
    "    power = epoch // 400\n",
    "    lr *= 0.8**power\n",
    "    return lr\n",
    "\n",
    "def unsupervised_labels(y, yp, n_classes, n_clusters): # Linear assignment algorithm\n",
    "    assert n_classes == n_clusters # Arguments: y (tensor): Ground truth labels // yp (tensor): Predicted clusters // n_classes (int): Number of classes // n_clusters (int): Number of clusters\n",
    "    C = np.zeros([n_clusters, n_classes]) # initialize count matrix\n",
    "    for i in range(len(y)): # populate count matrix\n",
    "        C[int(yp[i]), int(y[i])] += 1\n",
    "    row, col = linear_sum_assignment(-C) # optimal permutation using Hungarian Algo the higher the count, the lower the cost so we use -C for linear assignment\n",
    "    accuracy = C[row, col].sum() / C.sum() # compute accuracy\n",
    "    return accuracy * 100\n",
    "\n",
    "class AccuracyCallback(Callback): # Callback to compute the accuracy every epoch by calling the eval() method.\n",
    "    def __init__(self):\n",
    "        super(AccuracyCallback, self).__init__()\n",
    "        self.general_accuracy = 0\n",
    "    def on_epoch_end(self, epoch, logs=None): # Evaluate the accuracy of the current model weights\n",
    "        y_pred = my_model.predict(x_test)\n",
    "        for head in range(heads): # accuracy per head\n",
    "            if heads == 1:\n",
    "                y_head = y_pred\n",
    "            else:\n",
    "                y_head = y_pred[head]\n",
    "            y_head = np.argmax(y_head, axis=1)\n",
    "\n",
    "            accuracy = unsupervised_labels(list(y_test),list(y_head),n_labels,n_labels)\n",
    "            info = \"Head %d accuracy: %0.2f%%\"\n",
    "            if self.general_accuracy > 0:\n",
    "                info += \", Old best accuracy: %0.2f%%\"\n",
    "                data = (head, accuracy, self.general_accuracy)\n",
    "            else:\n",
    "                data = (head, accuracy)\n",
    "            print(info % data)\n",
    "            if accuracy > self.general_accuracy:\n",
    "                self.general_accuracy = accuracy # if accuracy improves during training, save the model weights on a file\n",
    "                    \n",
    "# Train function uses the data generator, accuracy computation, and learning rate scheduler callbacks        \n",
    "my_model.fit_generator(generator=train_gen,\n",
    "                            use_multiprocessing=False,\n",
    "                            epochs=epochs,\n",
    "                            callbacks=[AccuracyCallback(), LearningRateScheduler(lr_schedule,verbose=1)],\n",
    "                            workers=4,\n",
    "                            shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f0056b-9d10-461f-971f-c00da4184c14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
